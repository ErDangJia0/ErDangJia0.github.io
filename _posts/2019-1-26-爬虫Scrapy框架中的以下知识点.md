---
layout:     post
title:      爬虫Scrapy框架中的以下知识点
subtitle:   琐碎
date:       2019-1-26
author:     WHY
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - scrapy
---

## 1、extract() ：获取选择器对象中的文本内容

     response.xpath('')
     结果 ：[<selector ... data='文本内容'>,<...>]
     response.xpath('').extract()
     结果 ：['文本内容1','文本内容2',...]

## 2、pipelines.py中必须有1个函数叫:

     def process_item(self,item,spider):
        	 return item

必须返回item,其他管道会继续使用

## 3、日志级别

LOG_LEVEL = ''
    LOG_FILE = '文件名.log'
    5层日志级别
      1、CRITICAL ：严重错误
      2、ERROR    ：一般错误
      3、WARNING  ：警告信息(只显示WARNING、ERROR、CRITICAL信息)
      4、INFO     ：一般信息
      5、DEBUG    ：调试信息

## 4、保存为csv、json文件

##### 1、json文件

    scrapy crawl daomu -o daomu.json
    设置导出编码: settings.py添加变量
      FEED_EXPORT_ENCODING = 'utf-8'

##### 2、csv文件

    scrapy crawl daomu -o daomu.csv
    csv文件出现空行解决方法(修改源码)
    C:\Users\Python\Anaconda3\Lib\site-packages\scrapy\exporters.py 做如下修改(搜索csv类):
        self.stream = io.TextIOWrapper(
            file,
            newline='', # 添加此行